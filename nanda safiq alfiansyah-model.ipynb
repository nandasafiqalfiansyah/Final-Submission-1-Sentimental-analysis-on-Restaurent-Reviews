{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tfx\n",
        "!pip install autopep8\n",
        "!pip install pylint"
      ],
      "metadata": {
        "id": "R5XHyZhDdUGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir modules"
      ],
      "metadata": {
        "id": "MKrN-_hPh3VY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COMPONENTS_FILE = \"modules/components.py\"\n",
        "PIPELINE_FILE = \"modules/pipeline.py\"\n",
        "\n",
        "TRANSFORM_MODULE_FILE = \"modules/transform.py\"\n",
        "TRAINER_MODULE_FILE =  \"modules/trainer.py\""
      ],
      "metadata": {
        "id": "V-Ecq6Jef9kL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {COMPONENTS_FILE}\n",
        "import os\n",
        "import tensorflow_model_analysis as tfma\n",
        "\n",
        "from tfx.components import (\n",
        "    CsvExampleGen,\n",
        "    StatisticsGen,\n",
        "    SchemaGen,\n",
        "    ExampleValidator,\n",
        "    Transform,\n",
        "    Trainer,\n",
        "    Evaluator,\n",
        "    Pusher\n",
        ")\n",
        "from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2\n",
        "from tfx.types import Channel\n",
        "from tfx.dsl.components.common.resolver import Resolver\n",
        "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
        "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import (\n",
        "    LatestBlessedModelStrategy\n",
        ")\n",
        "\n",
        "\n",
        "def init_components(\n",
        "    data_dir,\n",
        "    transform_module,\n",
        "    training_module,\n",
        "    training_steps,\n",
        "    eval_steps,\n",
        "    serving_model_dir,\n",
        "):\n",
        "    \"\"\"\n",
        "    Initializes TFX components required for building a pipeline for training and deploying a model.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): The directory containing the input data.\n",
        "        transform_module (str): The path to the module containing the transformation logic.\n",
        "        training_module (str): The path to the module containing the training logic.\n",
        "        training_steps (int): The number of training steps.\n",
        "        eval_steps (int): The number of evaluation steps.\n",
        "        serving_model_dir (str): The directory where the trained model will be exported for serving.\n",
        "    \"\"\"\n",
        "\n",
        "    output = example_gen_pb2.Output(\n",
        "        split_config=example_gen_pb2.SplitConfig(splits=[\n",
        "            example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=8),\n",
        "            example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=2)\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    example_gen = CsvExampleGen(\n",
        "        input_base=data_dir,\n",
        "        output_config=output\n",
        "    )\n",
        "\n",
        "    statistics_gen = StatisticsGen(\n",
        "        examples=example_gen.outputs['examples']\n",
        "    )\n",
        "\n",
        "    schema_gen = SchemaGen(\n",
        "        statistics=statistics_gen.outputs[\"statistics\"]\n",
        "    )\n",
        "\n",
        "    example_validator = ExampleValidator(\n",
        "        statistics=statistics_gen.outputs['statistics'],\n",
        "        schema=schema_gen.outputs['schema']\n",
        "    )\n",
        "\n",
        "    transform = Transform(\n",
        "        examples=example_gen.outputs['examples'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "        module_file=os.path.abspath(transform_module)\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        module_file=os.path.abspath(training_module),\n",
        "        examples=transform.outputs['transformed_examples'],\n",
        "        transform_graph=transform.outputs['transform_graph'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "        train_args=trainer_pb2.TrainArgs(\n",
        "            splits=['train'],\n",
        "            num_steps=training_steps\n",
        "        ),\n",
        "        eval_args=trainer_pb2.EvalArgs(\n",
        "            splits=['eval'],\n",
        "            num_steps=eval_steps\n",
        "        )\n",
        "    )\n",
        "\n",
        "    model_resolver = Resolver(\n",
        "        strategy_class=LatestBlessedModelStrategy,\n",
        "        model=Channel(type=Model),\n",
        "        model_blessing=Channel(type=ModelBlessing)\n",
        "    ).with_id('Latest_blessed_model_resolver')\n",
        "\n",
        "    eval_config = tfma.EvalConfig(\n",
        "        model_specs=[tfma.ModelSpec(label_key='Liked')],\n",
        "        slicing_specs=[tfma.SlicingSpec()],\n",
        "        metrics_specs=[\n",
        "            tfma.MetricsSpec(metrics=[\n",
        "                tfma.MetricConfig(class_name='ExampleCount'),\n",
        "                tfma.MetricConfig(class_name='AUC'),\n",
        "                tfma.MetricConfig(class_name='FalsePositives'),\n",
        "                tfma.MetricConfig(class_name='TruePositives'),\n",
        "                tfma.MetricConfig(class_name='FalseNegatives'),\n",
        "                tfma.MetricConfig(class_name='TrueNegatives'),\n",
        "                tfma.MetricConfig(class_name='BinaryAccuracy',\n",
        "                    threshold=tfma.MetricThreshold(\n",
        "                        value_threshold=tfma.GenericValueThreshold(\n",
        "                            lower_bound={'value': 0.5}\n",
        "                        ),\n",
        "                        change_threshold=tfma.GenericChangeThreshold(\n",
        "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                            absolute={'value': 0.0001}\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "            ])\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    evaluator = Evaluator(\n",
        "        examples=example_gen.outputs['examples'],\n",
        "        model=trainer.outputs['model'],\n",
        "        baseline_model=model_resolver.outputs['model'],\n",
        "        eval_config=eval_config\n",
        "    )\n",
        "\n",
        "    pusher = Pusher(\n",
        "        model=trainer.outputs[\"model\"],\n",
        "        model_blessing=evaluator.outputs[\"blessing\"],\n",
        "        push_destination=pusher_pb2.PushDestination(\n",
        "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "                base_directory=serving_model_dir\n",
        "            )\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    components = (\n",
        "        example_gen,\n",
        "        statistics_gen,\n",
        "        schema_gen,\n",
        "        example_validator,\n",
        "        transform,\n",
        "        trainer,\n",
        "        model_resolver,\n",
        "        evaluator,\n",
        "        pusher\n",
        "    )\n",
        "\n",
        "    return components"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRu4RABhf2Fn",
        "outputId": "f9811ce1-eeab-497c-9db0-8cc640095bd2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modules/components.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {PIPELINE_FILE}\n",
        "import os\n",
        "from typing import Text\n",
        "\n",
        "from absl import logging\n",
        "from tfx.orchestration import metadata, pipeline\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "\n",
        "PIPELINE_NAME = 'restaurant-reviews-pipeline'\n",
        "\n",
        "DATA_ROOT = 'data'\n",
        "TRANSFORM_MODULE_FILE = 'modules/transform.py'\n",
        "TRAINER_MODULE_FILE = 'modules/trainer.py'\n",
        "\n",
        "OUTPUT_BASE = 'output'\n",
        "serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\n",
        "pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
        "metadata_path = os.path.join(pipeline_root, 'metadata.sqlite')\n",
        "\n",
        "\n",
        "def init_local_pipeline(\n",
        "    components, pipeline_root: Text\n",
        ") -> pipeline.Pipeline:\n",
        "    \"\"\"\n",
        "    Initialize a local TFX pipeline.\n",
        "\n",
        "    Args:\n",
        "        components: A dictionary of TFX components to be included in the pipeline.\n",
        "        pipeline_root: Root directory for pipeline output artifacts.\n",
        "\n",
        "    Returns:\n",
        "        A TFX pipeline.\n",
        "    \"\"\"\n",
        "    logging.info(f'Pipeline root set to: {pipeline_root}')\n",
        "    beam_args = [\n",
        "        '--direct_running_mode=multi_processing'\n",
        "        # 0 auto-detect based on the number of CPUs available\n",
        "        # duraing execution time\n",
        "        '----direct_num_workers=0'\n",
        "    ]\n",
        "\n",
        "    return pipeline.Pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=pipeline_root,\n",
        "        components=components,\n",
        "        enable_cache=True,\n",
        "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
        "            metadata_path\n",
        "        ),\n",
        "        eam_pipeline_args=beam_args\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    logging.set_verbosity(logging.INFO)\n",
        "\n",
        "    from modules.components import init_components\n",
        "    components = init_components(\n",
        "        DATA_ROOT,\n",
        "        training_module=TRAINER_MODULE_FILE,\n",
        "        transform_module=TRANSFORM_MODULE_FILE,\n",
        "        training_steps=5000,\n",
        "        eval_steps=1000,\n",
        "        serving_model_dir=serving_model_dir,\n",
        "    )\n",
        "\n",
        "    pipeline = init_local_pipeline(components, pipeline_root)\n",
        "    BeamDagRunner().run(pipeline=pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0-84VKDffQ8",
        "outputId": "c9f71e41-a488-439e-d728-eb3b79a62335"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modules/pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {TRANSFORM_MODULE_FILE}\n",
        "\n",
        "\"\"\"\n",
        "This module contains functions for transforming restaurant reviews data.\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "LABEL_KEY_NEW = \"Liked\"\n",
        "FEATURE_KEY_NEW = \"Review\"\n",
        "\n",
        "def transformed_name(key):\n",
        "    \"\"\"\n",
        "    Transform the given key.\n",
        "\n",
        "    Args:\n",
        "        key (str): Input key to transform.\n",
        "\n",
        "    Returns:\n",
        "        str: Transformed key.\n",
        "    \"\"\"\n",
        "    return key + \"_xf\"\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "    \"\"\"\n",
        "    Preprocess input data.\n",
        "\n",
        "    Args:\n",
        "        inputs (dict): Input data dictionary containing 'Liked' and 'Review' keys.\n",
        "\n",
        "    Returns:\n",
        "        dict: Transformed output data dictionary.\n",
        "    \"\"\"\n",
        "    outputs = {}\n",
        "    outputs[transformed_name(LABEL_KEY_NEW)] = tf.cast(inputs[LABEL_KEY_NEW], tf.int64)\n",
        "    outputs[transformed_name(FEATURE_KEY_NEW)] = tf.strings.lower(inputs[FEATURE_KEY_NEW])\n",
        "    return outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiLK7Di0mxpP",
        "outputId": "51853130-8bfb-4747-a77f-144bddc42eed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modules/transform.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {TRAINER_MODULE_FILE}\n",
        "\n",
        "\"\"\"\n",
        "This module contains functions for training a stress model using TensorFlow and TensorFlow Transform.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow.keras import layers\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs\n",
        "\n",
        "# Define constants\n",
        "LABEL_KEY = \"Liked\"\n",
        "FEATURE_KEY = \"Review\"\n",
        "EMBEDDING_DIM = 16\n",
        "\n",
        "# Function to rename transformed features\n",
        "def transformed_name(key):\n",
        "    \"\"\"\n",
        "    Transform the given key.\n",
        "\n",
        "    Args:\n",
        "        key (str): Input key to transform.\n",
        "\n",
        "    Returns:\n",
        "        str: Transformed key.\n",
        "    \"\"\"\n",
        "    return key + \"_xf\"\n",
        "\n",
        "# Function to read data from compressed TFRecord files\n",
        "def gzip_reader_fn(filenames):\n",
        "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
        "\n",
        "# Input function to create transformed features and batch data\n",
        "def input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=64):\n",
        "    \"\"\"\n",
        "    Create input function for training data.\n",
        "\n",
        "    Args:\n",
        "        file_pattern (str): File pattern for input data.\n",
        "        tf_transform_output (tensorflow_transform.TFTransformOutput): TensorFlow Transform output.\n",
        "        num_epochs (int): Number of epochs.\n",
        "        batch_size (int): Batch size.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: Input dataset.\n",
        "    \"\"\"\n",
        "    transform_feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=transform_feature_spec,\n",
        "        reader=gzip_reader_fn,\n",
        "        num_epochs=num_epochs,\n",
        "        label_key=transformed_name(LABEL_KEY)\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "# Text vectorization layer for tokenization and data standardization\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower_and_strip_punctuation\",\n",
        "    max_tokens=10000,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=100\n",
        ")\n",
        "\n",
        "# Function to build the machine learning model\n",
        "def model_builder():\n",
        "    \"\"\"\n",
        "    Build the machine learning model.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Compiled Keras model.\n",
        "    \"\"\"\n",
        "    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)\n",
        "    reshaped_input = tf.reshape(inputs, [-1])\n",
        "    x = vectorize_layer(reshaped_input)\n",
        "    x = layers.Embedding(10000, EMBEDDING_DIM, name=\"embedding\")(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = layers.Dense(32, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=tf.keras.optimizers.Adam(0.01),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# Function to preprocess raw request data for deployment\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        "    \"\"\"\n",
        "    Get serving function for TensorFlow Serving.\n",
        "\n",
        "    Args:\n",
        "        model (tf.keras.Model): Trained Keras model.\n",
        "        tf_transform_output (tensorflow_transform.TFTransformOutput): TensorFlow Transform output.\n",
        "\n",
        "    Returns:\n",
        "        Callable: Serve function for TensorFlow Serving.\n",
        "    \"\"\"\n",
        "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "    @tf.function\n",
        "    def serve_tf_examples_fn(serialized_tf_examples):\n",
        "        feature_spec = tf_transform_output.raw_feature_spec()\n",
        "        feature_spec.pop(LABEL_KEY)\n",
        "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "        return model(transformed_features)\n",
        "\n",
        "    return serve_tf_examples_fn\n",
        "\n",
        "# Function to run the training process\n",
        "def run_fn(fn_args: FnArgs) -> None:\n",
        "    \"\"\"\n",
        "    Run the training process.\n",
        "\n",
        "    Args:\n",
        "        fn_args (FnArgs): Function arguments.\n",
        "    \"\"\"\n",
        "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')\n",
        "\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=log_dir, update_freq='batch'\n",
        "    )\n",
        "\n",
        "    es = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_binary_accuracy', mode='max', verbose=1, patience=10\n",
        "    )\n",
        "    mc = tf.keras.callbacks.ModelCheckpoint(\n",
        "        fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True\n",
        "    )\n",
        "\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "\n",
        "    train_set = input_fn(fn_args.train_files, tf_transform_output, 10)\n",
        "    val_set = input_fn(fn_args.eval_files, tf_transform_output, 10)\n",
        "    vectorize_layer.adapt(\n",
        "        [j[0].numpy()[0] for j in [i[0][transformed_name(FEATURE_KEY)] for i in list(train_set)]]\n",
        "    )\n",
        "\n",
        "    model = model_builder()\n",
        "\n",
        "    model.fit(\n",
        "        x=train_set,\n",
        "        validation_data=val_set,\n",
        "        callbacks=[tensorboard_callback, es, mc],\n",
        "        steps_per_epoch=1000,\n",
        "        validation_steps=1000,\n",
        "        epochs=10\n",
        "    )\n",
        "\n",
        "    signatures = {\n",
        "        'serving_default':\n",
        "        _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(\n",
        "            tf.TensorSpec(\n",
        "                shape=[None],\n",
        "                dtype=tf.string,\n",
        "                name='examples'\n",
        "            )\n",
        "        )\n",
        "    }\n",
        "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8kUJ5iqoqXo",
        "outputId": "30da35d5-264e-4f99-99b7-13f46e4cd17b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modules/trainer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hp7bBqktbems"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "from modules.pipeline import init_local_pipeline\n",
        "from modules.components import init_components"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PIPELINE_NAME = 'restaurant-reviews-pipeline'\n",
        "\n",
        "DATA_ROOT = 'data'\n",
        "TRANSFORM_MODULE_FILE = 'modules/transform.py'\n",
        "TRAINER_MODULE_FILE = 'modules/trainer.py'\n",
        "\n",
        "OUTPUT_BASE = 'output'\n",
        "serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\n",
        "pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
        "metadata_path = os.path.join(pipeline_root, 'metadata.sqlite')"
      ],
      "metadata": {
        "id": "PqD-etBPo-p2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "components = init_components(\n",
        "    data_dir=DATA_ROOT,\n",
        "    transform_module=TRANSFORM_MODULE_FILE,\n",
        "    training_module=TRAINER_MODULE_FILE,\n",
        "    training_steps=5000,\n",
        "    eval_steps=1000,\n",
        "    serving_model_dir=serving_model_dir\n",
        ")\n",
        "\n",
        "pipeline = init_local_pipeline(components, pipeline_root)\n",
        "BeamDagRunner().run(pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hBN-VWcXiPTN",
        "outputId": "69fbaeab-71fe-4a02-a0f3-e81b826a99a2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/ops/readers.py:1086: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Review_xf (InputLayer)      [(None, 1)]               0         \n",
            "                                                                 \n",
            " tf.reshape (TFOpLambda)     (None,)                   0         \n",
            "                                                                 \n",
            " text_vectorization (TextVe  (None, 100)               0         \n",
            " ctorization)                                                    \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 100, 16)           160000    \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 16)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                1088      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 163201 (637.50 KB)\n",
            "Trainable params: 163201 (637.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            " 124/1000 [==>...........................] - ETA: 11s - loss: 0.5541 - binary_accuracy: 0.6690"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_binary_accuracy improved from -inf to 0.76119, saving model to output/restaurant-reviews-pipeline/Trainer/model/7/Format-Serving\n",
            "1000/1000 [==============================] - 5s 4ms/step - loss: 0.5518 - binary_accuracy: 0.6705 - val_loss: 0.6528 - val_binary_accuracy: 0.7612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:111: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refactoring Code dan Penilaian Kode"
      ],
      "metadata": {
        "id": "41d7pBTqCb-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!autopep8 --in-place --aggressive --aggressive modules/components.py modules/pipeline.py modules/transform.py modules/trainer.py\n",
        "!pylint modules/components.py modules/pipeline.py modules/transform.py modules/trainer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv_FEuWLCREh",
        "outputId": "27343e86-4da4-4b5c-9955-79ec9a4682ef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************* Module components\n",
            "modules/components.py:1:0: C0114: Missing module docstring (missing-module-docstring)\n",
            "modules/components.py:23:0: R0913: Too many arguments (6/5) (too-many-arguments)\n",
            "modules/components.py:23:0: R0914: Too many local variables (18/15) (too-many-locals)\n",
            "modules/components.py:43:13: E1101: Module 'tfx.proto.example_gen_pb2' has no 'Output' member (no-member)\n",
            "modules/components.py:44:21: E1101: Module 'tfx.proto.example_gen_pb2' has no 'SplitConfig' member (no-member)\n",
            "modules/components.py:45:12: E1101: Module 'tfx.proto.example_gen_pb2' has no 'SplitConfig' member (no-member)\n",
            "modules/components.py:46:12: E1101: Module 'tfx.proto.example_gen_pb2' has no 'SplitConfig' member (no-member)\n",
            "modules/components.py:79:19: E1101: Module 'tfx.proto.trainer_pb2' has no 'TrainArgs' member (no-member)\n",
            "modules/components.py:83:18: E1101: Module 'tfx.proto.trainer_pb2' has no 'EvalArgs' member (no-member)\n",
            "modules/components.py:131:25: E1101: Module 'tfx.proto.pusher_pb2' has no 'PushDestination' member (no-member)\n",
            "modules/components.py:132:23: E1101: Module 'tfx.proto.pusher_pb2' has no 'PushDestination' member (no-member)\n",
            "************* Module pipeline\n",
            "modules/pipeline.py:1:0: C0114: Missing module docstring (missing-module-docstring)\n",
            "modules/pipeline.py:21:4: W0621: Redefining name 'components' from outer scope (line 57) (redefined-outer-name)\n",
            "modules/pipeline.py:21:16: W0621: Redefining name 'pipeline_root' from outer scope (line 16) (redefined-outer-name)\n",
            "modules/pipeline.py:56:4: E0401: Unable to import 'modules.components' (import-error)\n",
            "************* Module trainer\n",
            "modules/trainer.py:3:0: C0301: Line too long (101/100) (line-too-long)\n",
            "modules/trainer.py:9:0: E0401: Unable to import 'tensorflow.keras' (import-error)\n",
            "modules/trainer.py:35:0: C0116: Missing function or method docstring (missing-function-docstring)\n",
            "\n",
            "------------------------------------------------------------------\n",
            "Your code has been rated at 4.58/10 (previous run: 4.58/10, +0.00)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "6LqYnzQgCi6Z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir('/content')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTct69y4DIxX",
        "outputId": "e27967ad-b635-469b-a7ad-bd05d6ecdd86"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'modules',\n",
              " 'output',\n",
              " '.ipynb_checkpoints',\n",
              " 'data',\n",
              " 'requirements.txt',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r content_folder.zip /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaAWDcNVDLy-",
        "outputId": "a8af2781-b961-48da-fd40-71946daa811e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/ (stored 0%)\n",
            "  adding: content/.config/ (stored 0%)\n",
            "  adding: content/.config/default_configs.db (deflated 98%)\n",
            "  adding: content/.config/gce (stored 0%)\n",
            "  adding: content/.config/config_sentinel (stored 0%)\n",
            "  adding: content/.config/.last_survey_prompt.yaml (stored 0%)\n",
            "  adding: content/.config/active_config (stored 0%)\n",
            "  adding: content/.config/.last_opt_in_prompt.yaml (stored 0%)\n",
            "  adding: content/.config/.last_update_check.json (deflated 22%)\n",
            "  adding: content/.config/logs/ (stored 0%)\n",
            "  adding: content/.config/logs/2024.04.18/ (stored 0%)\n",
            "  adding: content/.config/logs/2024.04.18/13.24.29.138246.log (deflated 58%)\n",
            "  adding: content/.config/logs/2024.04.18/13.24.58.719263.log (deflated 56%)\n",
            "  adding: content/.config/logs/2024.04.18/13.24.39.243263.log (deflated 86%)\n",
            "  adding: content/.config/logs/2024.04.18/13.24.02.104562.log (deflated 91%)\n",
            "  adding: content/.config/logs/2024.04.18/13.24.47.706251.log (deflated 58%)\n",
            "  adding: content/.config/logs/2024.04.18/13.24.58.057432.log (deflated 57%)\n",
            "  adding: content/.config/configurations/ (stored 0%)\n",
            "  adding: content/.config/configurations/config_default (deflated 15%)\n",
            "  adding: content/modules/ (stored 0%)\n",
            "  adding: content/modules/__pycache__/ (stored 0%)\n",
            "  adding: content/modules/__pycache__/pipeline.cpython-310.pyc (deflated 40%)\n",
            "  adding: content/modules/__pycache__/components.cpython-310.pyc (deflated 48%)\n",
            "  adding: content/modules/trainer.py (deflated 66%)\n",
            "  adding: content/modules/pipeline.py (deflated 60%)\n",
            "  adding: content/modules/transform.py (deflated 56%)\n",
            "  adding: content/modules/components.py (deflated 73%)\n",
            "  adding: content/output/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/SchemaGen/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/SchemaGen/schema/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/SchemaGen/schema/4/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/SchemaGen/schema/4/schema.pbtxt (deflated 55%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/SchemaGen/.system/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/SchemaGen/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/SchemaGen/.system/executor_execution/4/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model_run/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model_run/7/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model/7/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model/7/Format-Serving/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model/7/Format-Serving/variables/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model/7/Format-Serving/variables/variables.index (deflated 59%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model/7/Format-Serving/variables/variables.data-00000-of-00001 (deflated 65%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model/7/Format-Serving/fingerprint.pb (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model/7/Format-Serving/saved_model.pb (deflated 85%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model/7/Format-Serving/assets/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model/7/Format-Serving/keras_metadata.pb (deflated 85%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model/7/logs/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model/7/logs/validation/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model/7/logs/validation/events.out.tfevents.1713691856.7d5f973a6a94.1412.1.v2 (deflated 37%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model/7/logs/train/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/model/7/logs/train/events.out.tfevents.1713691852.7d5f973a6a94.1412.0.v2 (deflated 85%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/.system/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Trainer/.system/executor_execution/7/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Pusher/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Pusher/pushed_model/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Pusher/pushed_model/9/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Pusher/pushed_model/9/variables/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Pusher/pushed_model/9/variables/variables.index (deflated 59%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Pusher/pushed_model/9/variables/variables.data-00000-of-00001 (deflated 65%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Pusher/pushed_model/9/fingerprint.pb (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Pusher/pushed_model/9/saved_model.pb (deflated 85%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Pusher/pushed_model/9/assets/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Pusher/pushed_model/9/keras_metadata.pb (deflated 85%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Pusher/.system/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Pusher/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Pusher/.system/executor_execution/9/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/CsvExampleGen/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/CsvExampleGen/examples/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/CsvExampleGen/examples/2/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/CsvExampleGen/examples/2/Split-eval/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/CsvExampleGen/examples/2/Split-eval/data_tfrecord-00000-of-00001.gz (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/CsvExampleGen/examples/2/Split-train/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/CsvExampleGen/examples/2/Split-train/data_tfrecord-00000-of-00001.gz (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/CsvExampleGen/.system/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/CsvExampleGen/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/CsvExampleGen/.system/executor_execution/2/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/CsvExampleGen/.system/driver_execution/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/CsvExampleGen/.system/driver_execution/1713691810476428/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/metadata.sqlite (deflated 94%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/_wheels/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/_wheels/tfx_user_code_Transform-0.0+9fca0e2c163fd8f70794eb6ee9163b723be2e7639a95d48f8c5eb96a3e3ba7aa-py3-none-any.whl (deflated 16%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/_wheels/tfx_user_code_Trainer-0.0+9fca0e2c163fd8f70794eb6ee9163b723be2e7639a95d48f8c5eb96a3e3ba7aa-py3-none-any.whl (deflated 16%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/ExampleValidator/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/ExampleValidator/.system/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/ExampleValidator/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/ExampleValidator/.system/executor_execution/5/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/ExampleValidator/anomalies/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/ExampleValidator/anomalies/5/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/ExampleValidator/anomalies/5/Split-eval/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/ExampleValidator/anomalies/5/Split-eval/SchemaDiff.pb (deflated 28%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/ExampleValidator/anomalies/5/Split-train/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/ExampleValidator/anomalies/5/Split-train/SchemaDiff.pb (deflated 28%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Evaluator/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Evaluator/evaluation/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Evaluator/evaluation/8/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Evaluator/evaluation/8/plots-00000-of-00001.tfrecord (deflated 17%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Evaluator/evaluation/8/validations.tfrecord (deflated 8%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Evaluator/evaluation/8/metrics-00000-of-00001.tfrecord (deflated 43%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Evaluator/evaluation/8/attributions-00000-of-00001.tfrecord (deflated 17%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Evaluator/evaluation/8/eval_config.json (deflated 66%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Evaluator/.system/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Evaluator/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Evaluator/.system/executor_execution/8/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Evaluator/blessing/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Evaluator/blessing/8/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Evaluator/blessing/8/BLESSED (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/StatisticsGen/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/StatisticsGen/statistics/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/StatisticsGen/statistics/3/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/StatisticsGen/statistics/3/Split-eval/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/StatisticsGen/statistics/3/Split-eval/FeatureStats.pb (deflated 57%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/StatisticsGen/statistics/3/Split-train/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/StatisticsGen/statistics/3/Split-train/FeatureStats.pb (deflated 58%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/StatisticsGen/.system/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/StatisticsGen/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/StatisticsGen/.system/executor_execution/3/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transform_graph/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transform_graph/6/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transform_graph/6/transformed_metadata/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transform_graph/6/transformed_metadata/schema.pbtxt (deflated 54%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transform_graph/6/transform_fn/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transform_graph/6/transform_fn/variables/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transform_graph/6/transform_fn/variables/variables.index (deflated 33%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transform_graph/6/transform_fn/variables/variables.data-00000-of-00001 (deflated 22%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transform_graph/6/transform_fn/fingerprint.pb (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transform_graph/6/transform_fn/saved_model.pb (deflated 73%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transform_graph/6/transform_fn/assets/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transform_graph/6/metadata/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transform_graph/6/metadata/schema.pbtxt (deflated 55%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/post_transform_anomalies/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/post_transform_anomalies/6/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/post_transform_anomalies/6/SchemaDiff.pb (deflated 28%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/post_transform_schema/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/post_transform_schema/6/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/post_transform_schema/6/schema.pbtxt (deflated 54%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transformed_examples/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transformed_examples/6/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transformed_examples/6/Split-eval/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transformed_examples/6/Split-eval/transformed_examples-00000-of-00001.gz (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transformed_examples/6/Split-train/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/transformed_examples/6/Split-train/transformed_examples-00000-of-00001.gz (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/updated_analyzer_cache/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/updated_analyzer_cache/6/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/pre_transform_schema/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/pre_transform_schema/6/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/pre_transform_schema/6/schema.pbtxt (deflated 55%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/pre_transform_stats/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/pre_transform_stats/6/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/pre_transform_stats/6/FeatureStats.pb (deflated 58%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/.system/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/.system/executor_execution/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/.system/executor_execution/6/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/post_transform_stats/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/post_transform_stats/6/ (stored 0%)\n",
            "  adding: content/output/restaurant-reviews-pipeline/Transform/post_transform_stats/6/FeatureStats.pb (deflated 59%)\n",
            "  adding: content/output/serving_model/ (stored 0%)\n",
            "  adding: content/output/serving_model/1713691875/ (stored 0%)\n",
            "  adding: content/output/serving_model/1713691875/variables/ (stored 0%)\n",
            "  adding: content/output/serving_model/1713691875/variables/variables.index (deflated 59%)\n",
            "  adding: content/output/serving_model/1713691875/variables/variables.data-00000-of-00001 (deflated 65%)\n",
            "  adding: content/output/serving_model/1713691875/fingerprint.pb (stored 0%)\n",
            "  adding: content/output/serving_model/1713691875/saved_model.pb (deflated 85%)\n",
            "  adding: content/output/serving_model/1713691875/assets/ (stored 0%)\n",
            "  adding: content/output/serving_model/1713691875/keras_metadata.pb (deflated 85%)\n",
            "  adding: content/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/data/ (stored 0%)\n",
            "  adding: content/data/Restaurant_Reviews.csv (deflated 61%)\n",
            "  adding: content/requirements.txt (deflated 54%)\n",
            "  adding: content/sample_data/ (stored 0%)\n",
            "  adding: content/sample_data/README.md (deflated 42%)\n",
            "  adding: content/sample_data/anscombe.json (deflated 83%)\n",
            "  adding: content/sample_data/mnist_test.csv (deflated 88%)\n",
            "  adding: content/sample_data/mnist_train_small.csv (deflated 88%)\n",
            "  adding: content/sample_data/california_housing_test.csv (deflated 76%)\n",
            "  adding: content/sample_data/california_housing_train.csv (deflated 79%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('content_folder.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nxp2el7xDOx_",
        "outputId": "3f3c3302-974e-41a5-fc93-6aeb01741d0c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1700a95a-5ece-455d-bed0-851de5ec37f2\", \"content_folder.zip\", 9520551)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}